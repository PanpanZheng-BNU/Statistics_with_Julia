{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing Data\n",
    "## Single Sample\n",
    "> Given a set of observations, $x_{1},x_{2},\\cdots,x_{n}$\n",
    "\n",
    "- The ***sample mean*** is the informative ***measure of centrality.***. It is denoted by $\\bar{x}$\n",
    "$$\n",
    "\\bar{x} = \\frac{\\displaystyle{\\sum_{i=1}^{n}x_{i}}}{n}.\n",
    "$$\n",
    "- ***Geometric mean*** and ***harmonic mean*** \n",
    "$$\n",
    "\\bar{x}_{g} = \\sqrt[n]{\\prod_{i=1}^{n}x_{i}} \\quad \\text{and}\\quad \\bar{x}_{h} = \\frac{n}{\\displaystyle{\\sum_{i=i}^{n}\\frac{1}{x_{i}}}}\n",
    "$$\n",
    "  - The geometric mean is useful for averaging growths factors. For example, if $x_{1}=1.03$, $x_{2}=1.05$, $x_{3} = 1.07$ are growth factors, the geometric mena, $\\bar{x}_{g} = 1.049714$ is a good summary stastic of the \"average growth factor\". This is because the growth factor obtained by $\\bar{x}_{g}^{3}$ equals the growth factor $x_{1}\\cdot x_{2}\\cdot x_{3}$, such as:\n",
    "$$\n",
    "\\text{Value after three periods} = 100\\cdot x_{1}\\cdot x_{2} \\cdot x_{3} = 100\\cdot \\bar{x}_{g}^{3} = 115.7205\n",
    "$$\n",
    "  - The harmonic mean is useful for averaging rates or speeds. For example assume that you are on a brisk hike, walking 5km up a mountain and then 5km back down. Say your speed going up is $x_{1} = 5\\text{ km/h}$ and your speed going down is $x_{2} = 10\\text{ km/h}$. What is your \"average speed\" for the whole journey? You travel up for $1 \\text{ h}$ and down for $0.5 \\text{ h}$ and hence your total travel time is $1.5 \\text{ h}$. Hence the average speed is $10/1.5 = 6.6\\bar{6}\\text{ km/h} = \\frac{2}{\\frac{1}{5}+\\frac{1}{10}}$.\n",
    "- Note that for any dataset $\\bar{x}_{h}\\le \\bar{x}_{g}\\le \\bar{x}$. Here, the inequalities become equalities only if all observations are equal.\n",
    "- A different breed of descriptive statistics is based on ***order statistics***. This term is used to describe the ***sorted sample***, and is sometimes denoted by\n",
    "$$\n",
    "x_{(1)} \\le x_{(2)} \\le \\cdots \\le x_{(n)}\n",
    "$$\n",
    "  - Based on the order statistics, we can define a variety of statistics such as the ***minimum***, $x_{(1)}$, the ***maximum***, $x_{(n)}$, and the ***median***, which in the case of $n$ beging odd is $x_{((n+1)/2)}$ and in case of $n$ being even is the arithmetic mean of $x_{(n/2)}$ and $x_{(n/2+1)}$.\n",
    "  - Related statistics are the ${\\alpha-quantile}$, for $\\alpha\\in[0,1]$ which is effectively $x_{\\tilde{an}}$, where $\\tilde{an}$ denotes a rounding of $an$ to the nearest element of $\\{1,\\cdots,n\\}$. For $\\alpha=0.25$ and $\\alpha=0.75$, these values are known as the ***first quartile*** and ***third quartile*** respectively.\n",
    "- The ***inter quartile range (IQR)*** is the difference between these two quartiles and the ***range*** is $x_{( n )}-x_{( 1 )}$, which are the ***measure of dispersion***. When dealing with measures of dispersion, the most popular and useful measure is the ***sample variance***, ($\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}{n}$ is called as ***pupulation variance***)\n",
    "$$\n",
    "s^{2} = \\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}}{n-1} = \\frac{\\sum_{i=1}^{n}x_{i}^{2}-n \\bar{x}^{2}}{n-1}\n",
    "$$\n",
    "  - If all observations are constant then $s^{2}=0$, otherwise, $s^{2}>0$, and the bigger it is, the more dispersion we have in the data.\n",
    "  - A related quantity is the ***sample standard deviation*** $s = \\sqrt{s^{2}}$\n",
    "  - ***Standard error*** $\\text{S.E.} = s/\\sqrt{n}$\n",
    "- Compute in Julia\n",
    "  - `var(a::Array)`: Sample variance\n",
    "  - `std(a::Array)`: Sample standard deviation\n",
    "  - `iqr(a::Array)`: Interquartile range \n",
    "  - `percentile(a::Array, n::Int)`: Get the `n`th percentile of data\n",
    "  - `quantile(a:Array, n::Float)`: Get the `n` quantile of data\n",
    "  - `summarystats(a::Array)`: Get the basic descrptive statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Mean: 27.155405405405407\n",
      "Harmonic <= Geometric <= Arithemetic (26.52181504074163, 26.84561900212437, 27.155405405405407)\n",
      "Sample Variance: 16.125389558372802\n",
      "Sample Standard Deviation: 4.015643106449177\n",
      "Minimum: 16.1\n",
      "Maximum: 37.6\n",
      "Median: 27.7\n",
      "95th percentile: 33.0\n",
      "0.95 quantile: 33.0\n",
      "Interquartile range: 6.100000000000001\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Summary Stats:\n",
       "Length:         777\n",
       "Missing Count:  0\n",
       "Mean:           27.155405\n",
       "Minimum:        16.100000\n",
       "1st Quartile:   24.000000\n",
       "Median:         27.700000\n",
       "3rd Quartile:   30.100000\n",
       "Maximum:        37.600000\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV, Statistics, StatsBase, DataFrames\n",
    "data = CSV.read(\"../data/temperatures.csv\", DataFrame)[:,4]\n",
    "\n",
    "println(\"Sample Mean: \", mean(data))\n",
    "println(\"Harmonic <= Geometric <= Arithemetic \",\n",
    "    (harmmean(data), geomean(data), mean(data)))\n",
    "println(\"Sample Variance: \", var(data))\n",
    "println(\"Sample Standard Deviation: \", std(data))\n",
    "println(\"Minimum: \", minimum(data))\n",
    "println(\"Maximum: \", maximum(data))\n",
    "println(\"Median: \", median(data))\n",
    "println(\"95th percentile: \", percentile(data, 95))\n",
    "println(\"0.95 quantile: \", quantile(data, 0.95))\n",
    "println(\"Interquartile range: \", iqr(data), \"\\n\")\n",
    "\n",
    "summarystats(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations in Pairs\n",
    "> Data is configured iin the form of pairs, $(x_{1}, y_{1}), \\cdots, (x_{n},y_{n})$\n",
    "\n",
    "- We often consider the ***sample covariance***, which is given by,\n",
    "$$\n",
    "\\widehat{\\text{cov}}_{x,y} = \\frac{\\displaystyle{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}}{n-1}\n",
    "$$\n",
    "   - A positive covariance indicates a **positive linear relationship** meaning that when $x$ is larger than its means, we expect $y$ to be larger than its mean, and similarly when $x$ is small then $y$ is small.\n",
    "   - A negative covariance indicates a **negative linear relationship** meaning that hwen $x$ is large then $y$ is small.\n",
    "   - If the covariance is $0$ or near $0$, it is an indication that no such relationship holds.\n",
    "- However, like the variance, the covariance is not a normalized quantity. For this reason, we define another useful statistics, the ***sample correlation coefficient*** \n",
    "$$\n",
    "\\hat{\\rho}_{x,y} =  \\frac{\\widehat{\\text{cov}}_{x,y}}{s_{x}s_{y}}\n",
    "$$\n",
    "  - $\\hat{\\rho}_{x,y} \\in [0,1]$, The sign of $\\hat{\\rho}_{x,y}$ agrees with the sign of $\\widehat{\\text{cov}}_{x,y}$, however importantly its magnitude is meaningful. Having $\\vert \\hat{\\rho}_{x,y} \\vert$ near $0$ implies little or no linear relationship, while $\\vert \\hat{\\rho}_{x,y} \\vert$ closer to $1$ implies a stronger linear relationship, which is either positive or negative dependinig on the sign of $\\hat{\\rho}_{x,y}$.\n",
    "- It is often useful to represent the variances and covariances in the ***sample covariance matrix***  as:\n",
    "$$\n",
    "\\hat{\\Sigma} = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "  \\widehat{\\text{cov}}_{x,x} & \\widehat{\\text{cov}}_{x,y} \\\\\n",
    "  \\widehat{\\text{cov}}_{x,y} & \\widehat{\\text{cov}}_{y,y} \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "= \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "  s_{x}^{2} & \\hat{\\rho}_{x,y}s_{x}s_{y} \\\\\n",
    "  \\hat{\\rho}_{x,y}s_{x}s_{y} & s_{y}^{2}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meanVect = [27.155405405405407, 26.163835263835264] \n",
      "covMat = [16.1253895583728 13.046961200891614; 13.046961200891614 12.367253570765161]\n"
     ]
    }
   ],
   "source": [
    "using DataFrames, CSV, Statistics\n",
    "\n",
    "data = CSV.read(\"../data/temperatures.csv\", DataFrame, copycols=true)\n",
    "brisT = data.Brisbane\n",
    "gcT = data.GoldCoast\n",
    "\n",
    "sigB = std(brisT)\n",
    "sigG = std(gcT)\n",
    "covBG = cov(brisT, gcT)\n",
    "\n",
    "meanVect = [mean(brisT), mean(gcT)]\n",
    "covMat = [sigB^2 covBG;\n",
    "    covBG sigG^2]\n",
    "\n",
    "outfile = open(\"../data/mvParams.jl\", \"w\")\n",
    "write(outfile, \"meanVect = $meanVect \\ncovMat = $covMat\")\n",
    "close(outfile)\n",
    "println(read(\"../data/mvParams.jl\", String))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations in Vectors\n",
    "> Considering data that consists of $n$ vectors. The $i$'th data vector represents a tuple of values, $(x_{i1}, \\cdots, x_{ip})$. In this case, the data can be represented by a $n\\times p $ ***data matrix***, $\\pmb{X}$, where the rows are observations (data vectors) and each column represents a different ***variable***, ***feature*** or ***attribute*** \n",
    "\n",
    "- In summarizing the data $\\pmb{X}$, a few basic objects arise. These include the ***sample mean vector***, ***sample standard deviation vector***, ***sample covariance matric*** and the ***sample correlation matrix***.\n",
    "  - The ***sample mean vector*** is simply a vector of length $p$ where the $j$'th entry, $\\bar{x}_{j}$ is the sample mean of $(x_{1j}, \\cdots, x_{nj})$, based on the $j$'th column of $\\pmb{X}$.\n",
    "  - The ***sample standard deviation vector*** has a $j$'th entries, $s_{j}$, which is the sample standard deviation of $(x_{1j},\\cdots,x_{nj})$\n",
    "- With these we often ***standardize*** the data by creating a new $n\\times p$ matrix $Z$, with entries,\n",
    "$$\n",
    "Z_{ij} = \\frac{x_{ij} - \\bar{x}_{j}}{s_{j}}, \\quad i = 1,\\cdots,n, \\quad j = 1,\\cdots, p.\n",
    "$$\n",
    "  - It can be creat via, $\\pmb{Z} = (\\pmb{X} - \\pmb{1 \\bar{x}}^{\\top})\\cdot \\text{diag}(\\pmb{s})^{-1}$\n",
    "    - Where $\\pmb{1}$ is a column vector of 1's ($n$ rows)\n",
    "    - $\\pmb{\\bar{x}}$ is the mean vector\n",
    "    - $\\text{diag}(\\pmb{s})$ is a diagonal matrix which is created from standard deviation vector $\\pmb{s}$\n",
    "  - The standarized data has the attribute that each column, $(z_{1j},\\cdots,z_{nj})^{\\top}$, has a $0$ sample mean and a unit standard deviation. Hence, the first- and second-order information of the $j$'th feature is lost when moving from the data matrix $\\pmb{X}$ to the standarized matrix $\\pmb{Z}$. Nevertheless, relationships between features are still captured in $\\pmb{Z}$ and can be easily calculated. Most notably, the sample correlation between feature $j_{1}$ and feather $j_{2}$, denoted by $\\hat{\\rho}_{j_{1},j_{2}}$ is simply calculatd via\n",
    "$$\n",
    "\\hat{\\rho}_{j_{1},j_{2}} = \\frac{\\sum_{i=1}^{n}z_{ij1}z_{ij2}}{n-1} = \\bigg[\\frac{1}{n-1}\\pmb{Z}^{\\top}\\pmb{Z}\\bigg]_{j_{1},j_{2}}\n",
    "$$\n",
    "  - Without resorting to standarization, it is often of interest to calculate the $p\\times p $ ***sample covariance matrix*** $\\hat{\\Sigma}$\n",
    "$$\n",
    "\\hat{\\pmb{\\Sigma}} = \\frac{1}{n-1}(\\pmb{X} - \\pmb{1 \\bar{x}}^{\\top})^{\\top}(\\pmb{X} - \\pmb{1 \\bar{x}}^{\\top}) = \\frac{1}{n-1}\\pmb{X}^{\\top}(\\pmb{I} - n^{-1}\\pmb{11}^{\\top})\\pmb{X}\n",
    "$$\n",
    "- ***proof*** \n",
    "$$\n",
    "     \\begin{array}{ll}\n",
    "       \\because & \\pmb{\\bar{x}} = n^{-1}\\pmb{1}^{\\top}\\pmb{X} \\\\\n",
    "       \\therefore & \\hat{\\pmb{\\Sigma}} = (\\pmb{X} - n^{-1}\\pmb{11}^{\\top}\\pmb{X})^{\\top}(\\pmb{X} - n^{-1}\\pmb{11}^{\\top}\\pmb{X}) \\\\\n",
    "       & = (\\pmb{X}^{\\top} - \\pmb{X}^{\\top}n^{-1}\\pmb{11}^{\\top})(\\pmb{X} - n^{-1}\\pmb{11}^{\\top}\\pmb{X}) \\\\\n",
    "       & = \\pmb{X}^{\\top}(\\pmb{I} - n^{-1}\\pmb{11}^{\\top})(\\pmb{I} - n^{-1}\\pmb{11}^{\\top})\\pmb{X} \\\\\n",
    "       & = \\pmb{X}^{\\top}(\\pmb{I} - 2n^{-1}\\pmb{11}^{\\top} + n^{-2}\\pmb{11}^{\\top}\\pmb{11}^{\\top}) \\pmb{X} \\\\\n",
    "       \\because & \\pmb{1}^{\\top}\\pmb{1} = n \\\\\n",
    "       \\therefore & \\hat{\\pmb{\\Sigma}} = \\pmb{X}^{\\top}(\\pmb{I} - n^{-1}\\pmb{11}^{\\top}) \\pmb{X} \\\\\n",
    "     \\end{array}\n",
    "$$\n",
    "- Compute in Julia\n",
    "  - `zscore(a::Array)`: convert array to z-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3\n",
      "Number of observations: 7\n",
      "Dimensions of data matrix: (7, 3)\n",
      "\n",
      "Alernative calculations of (sample) mean vector: \n",
      "xbarA = [1.057142857142857, 2.0857142857142854, 3.4999999999999996]\n",
      "xbarB = [1.0571428571428572, 2.085714285714286, 3.5]\n",
      "xbarC = [1.0571428571428572 2.085714285714286 3.5]\n",
      "Y is the de-meaned data: [1.5860328923216522e-16 2.854859206178974e-16 7.61295788314393e-16]\n",
      "\n",
      "Alernative calculations of (sample) covariance matrix: \n",
      "covA = [0.11952380952380952 -0.08738095238095239 0.43999999999999995; -0.08738095238095239 0.12142857142857146 -0.7149999999999999; 0.43999999999999995 -0.7149999999999999 8.033333333333333]\n",
      "covB = [0.11952380952380955 -0.0873809523809524 0.4400000000000001; -0.0873809523809524 0.12142857142857143 -0.7150000000000002; 0.4400000000000001 -0.7150000000000002 8.033333333333337]\n",
      "covC = [0.11952380952380952 -0.0873809523809524 0.44; -0.0873809523809524 0.12142857142857146 -0.715; 0.44 -0.715 8.033333333333335]\n",
      "covD = [0.11952380952380953 -0.0873809523809524 0.44; -0.08738095238095242 0.12142857142857147 -0.7150000000000001; 0.44000000000000006 -0.7150000000000001 8.033333333333335]\n",
      "covE = [0.1195238095238095 -0.0873809523809524 0.44; -0.0873809523809524 0.12142857142857146 -0.715; 0.44 -0.715 8.033333333333335]\n",
      "\n",
      "Alernative computation of z-score yieds same matrix: (2.220446049250313e-16, 3.6996474861485986e-15, 3.772231884986008e-15)\n",
      "\n",
      "Alernative calculations of (sample) correlation matrix: \n",
      "corA = [0.9999999999999999 -0.7253191060768938 0.4490322867507891; -0.7253191060768938 0.9999999999999999 -0.723931884701913; 0.4490322867507891 -0.723931884701913 0.9999999999999998]\n",
      "corB = [0.9999999999999999 -0.7253191060768938 0.4490322867507891; -0.7253191060768938 0.9999999999999999 -0.723931884701913; 0.4490322867507891 -0.723931884701913 0.9999999999999998]\n",
      "corC = [1.0 -0.725319106076894 0.4490322867507892; -0.725319106076894 1.0 -0.7239318847019133; 0.4490322867507892 -0.7239318847019133 1.0]\n",
      "corD = [0.9999999999999999 -0.7253191060768938 0.4490322867507892; -0.7253191060768938 0.9999999999999999 -0.7239318847019133; 0.4490322867507892 -0.7239318847019133 1.0000000000000002]\n",
      "corE = [0.9999999999999998 -0.7253191060768939 0.4490322867507892; -0.7253191060768939 1.0 -0.7239318847019132; 0.4490322867507892 -0.7239318847019132 1.0]\n",
      "corF = [1.0 -0.725319106076894 0.4490322867507892; -0.725319106076894 1.0 -0.7239318847019133; 0.4490322867507892 -0.7239318847019133 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.9999999999999999 -0.7253191060768938 0.4490322867507891; -0.7253191060768938 0.9999999999999999 -0.723931884701913; 0.4490322867507891 -0.723931884701913 0.9999999999999998], [0.9999999999999999 -0.7253191060768938 0.4490322867507891; -0.7253191060768938 0.9999999999999999 -0.723931884701913; 0.4490322867507891 -0.723931884701913 0.9999999999999998], [1.0 -0.725319106076894 0.4490322867507892; -0.725319106076894 1.0 -0.7239318847019133; 0.4490322867507892 -0.7239318847019133 1.0], [0.9999999999999999 -0.7253191060768938 0.4490322867507892; -0.7253191060768938 0.9999999999999999 -0.7239318847019133; 0.4490322867507892 -0.7239318847019133 1.0000000000000002], [0.9999999999999998 -0.7253191060768939 0.4490322867507892; -0.7253191060768939 1.0 -0.7239318847019132; 0.4490322867507892 -0.7239318847019132 1.0], [1.0 -0.725319106076894 0.4490322867507892; -0.725319106076894 1.0 -0.7239318847019133; 0.4490322867507892 -0.7239318847019133 1.0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Statistics, StatsBase, LinearAlgebra, DataFrames, CSV\n",
    "df = CSV.read(\"../data/3featureData.csv\",DataFrame ,header=false)\n",
    "n,p = size(df)\n",
    "println(\"Number of features: \", p)\n",
    "println(\"Number of observations: \", n)\n",
    "X = Matrix{Float64}(df)\n",
    "println(\"Dimensions of data matrix: \", size(X))\n",
    "\n",
    "xbarA = (1/n) * X' * ones(n)\n",
    "xbarB = [mean(X[:,i]) for i in 1:p]\n",
    "xbarC = sum(X, dims=1) ./ n\n",
    "println(\"\\nAlernative calculations of (sample) mean vector: \")\n",
    "@show(xbarA), @show(xbarB), @show(xbarC)\n",
    "\n",
    "Y = (I - ones(n,n)/n) * X\n",
    "println(\"Y is the de-meaned data: \", mean(Y, dims=1))\n",
    "\n",
    "covA = (X .- xbarA')'*(X .- xbarA')/(n-1)\n",
    "covB = Y'*Y/(n-1)\n",
    "covC = [cov(X[:,i],X[:, j]) for i in 1:p, j in 1:p]\n",
    "covD = [cor(X[:,i],X[:, j]) * std(X[:,i]) * std(X[:,j]) for i in 1:p, j in 1:p]\n",
    "covE = cov(X)\n",
    "println(\"\\nAlernative calculations of (sample) covariance matrix: \")\n",
    "@show(covA), @show(covB), @show(covC), @show(covD), @show(covE)\n",
    "\n",
    "ZmatA = [(X[i,j] - xbarA[j])/sqrt(covA[j,j]) for i in 1:n, j in 1:p]\n",
    "ZmatB = (X .- xbarA') * sqrt.(Diagonal(covA))^(-1)\n",
    "ZmatC = hcat([zscore(X[:,j]) for j in 1:p]...)\n",
    "println(\"\\nAlernative computation of z-score yieds same matrix: \",\n",
    "    (maximum(norm(ZmatA - ZmatB)), maximum(norm(ZmatC - ZmatB)), maximum(norm(ZmatA - ZmatC))))\n",
    "Z = ZmatA\n",
    "\n",
    "corA = covA ./ [std(X[:,i])*std(X[:,j]) for i in 1:p, j in 1:p]\n",
    "corB = covA ./ (std(X, dims=1)' * std(X, dims=1))\n",
    "corC = [cor(X[:,i], X[:,j]) for i in 1:p, j in 1:p]\n",
    "corD = Z' * Z ./ (n-1)\n",
    "corE = cov(Z)\n",
    "corF = cor(X)\n",
    "println(\"\\nAlernative calculations of (sample) correlation matrix: \")\n",
    "@show(corA), @show(corB), @show(corC), @show(corD), @show(corE), @show(corF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
